"""
Amazon Search Query Performance Analyzer
Streamlit Haupt-App - Alle Sections auf einer Seite
"""
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from utils.data_processor import DataProcessor
from utils.ai_categorizer import AICategorizer
from utils.visualizations import VisualizationEngine
from config import APP_TITLE, APP_ICON
import time


# Page Config
st.set_page_config(
    page_title=APP_TITLE,
    page_icon=APP_ICON,
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Custom CSS
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    </style>
""", unsafe_allow_html=True)

# Initialize session state
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None
if 'all_processed_data' not in st.session_state:
    st.session_state.all_processed_data = []  # Liste aller hochgeladenen DatensÃ¤tze
if 'categorized_data' not in st.session_state:
    st.session_state.categorized_data = None
if 'categories' not in st.session_state:
    st.session_state.categories = None
if 'processor' not in st.session_state:
    st.session_state.processor = DataProcessor()
if 'categorizer' not in st.session_state:
    st.session_state.categorizer = AICategorizer()
if 'viz_engine' not in st.session_state:
    st.session_state.viz_engine = VisualizationEngine()

# Header
st.markdown(f'<div class="main-header">{APP_ICON} {APP_TITLE}</div>', unsafe_allow_html=True)

st.info("""
**Privacy First**: Alle Daten werden nur in deiner Browser-Session verarbeitet.
Keine Daten werden gespeichert oder an externe Server gesendet (auÃŸer ChatGPT API fÃ¼r Kategorisierung).
""")

# ============================================================================
# SECTION 1: DATA INGESTION
# ============================================================================
st.header("ðŸ“¤ Smart Ingestion Engine")

st.markdown("""
### Drag & Drop Upload
Lade deine Amazon Search Query Performance Reports hoch (.csv oder .xlsx).

**Mehrere Monate kombinieren**: Du kannst alle monatlichen Reports auf einmal auswÃ¤hlen und hochladen - sie werden automatisch kombiniert!
""")

# Button zum ZurÃ¼cksetzen aller Daten
if st.session_state.processed_data is not None:
    col1, col2 = st.columns([3, 1])
    with col2:
        if st.button("ðŸ—‘ï¸ Alle Daten zurÃ¼cksetzen", type="secondary"):
            st.session_state.all_processed_data = []
            st.session_state.processed_data = None
            st.session_state.categorized_data = None
            st.rerun()

uploaded_files = st.file_uploader(
    "WÃ¤hle Dateien aus",
    type=['csv', 'xlsx', 'xls'],
    accept_multiple_files=True,
    help="UnterstÃ¼tzt CSV und Excel Dateien"
)

if uploaded_files:
    with st.spinner("Verarbeite Dateien..."):
        try:
            # Lade alle Dateien (einzeln oder mehrere auf einmal)
            if len(uploaded_files) == 1:
                df = st.session_state.processor.load_file(uploaded_files[0])
                st.success(f"âœ… Datei geladen: {uploaded_files[0].name}")
            else:
                df = st.session_state.processor.load_multiple_files(uploaded_files)
                st.success(f"âœ… {len(uploaded_files)} Dateien geladen und kombiniert")
            
            # Daten bereinigen
            df_cleaned = st.session_state.processor.clean_data(df)
            
            # Kombiniere mit bereits vorhandenen Daten (falls vorhanden)
            if st.session_state.processed_data is not None:
                # Kombiniere neue Daten mit bestehenden
                df_combined = pd.concat([st.session_state.processed_data, df_cleaned], ignore_index=True)
                # Entferne Duplikate basierend auf Search Query + Month
                search_col = 'Search Query' if 'Search Query' in df_combined.columns else 'Search Term'
                if 'Month' in df_combined.columns and search_col in df_combined.columns:
                    df_combined = df_combined.drop_duplicates(subset=[search_col, 'Month'], keep='last')
                elif search_col in df_combined.columns:
                    df_combined = df_combined.drop_duplicates(subset=[search_col], keep='last')
                st.session_state.processed_data = df_combined
                st.info(f"ðŸ“Š Neue Daten hinzugefÃ¼gt. Total: {len(df_combined)} Zeilen aus allen hochgeladenen Reports")
            else:
                st.session_state.processed_data = df_cleaned
            
            # Zeige Zusammenfassung
            st.subheader("ðŸ“ˆ DatenÃ¼bersicht")
            stats = st.session_state.processor.get_summary_stats(df_cleaned)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total Queries", f"{stats['total_queries']:,}")
            with col2:
                st.metric("Total Impressions", f"{stats['total_impressions']:,.0f}")
            with col3:
                st.metric("Total Orders", f"{stats['total_orders']:,}")
            with col4:
                st.metric("Total Sales", f"â‚¬{stats['total_sales']:,.2f}")
            
            # Zeige Vorschau
            st.subheader("ðŸ‘€ Datenvorschau")
            st.dataframe(st.session_state.processed_data.head(20), use_container_width=True)
            
            # Monatliche Trend-Analyse
            if 'Month' in st.session_state.processed_data.columns or 'Reporting Date' in st.session_state.processed_data.columns:
                st.subheader("ðŸ“… Monatliche Trend-Analyse")
                try:
                    # PrÃ¼fe ob Methode existiert
                    if hasattr(st.session_state.viz_engine, 'create_monthly_trends'):
                        fig_monthly = st.session_state.viz_engine.create_monthly_trends(
                            st.session_state.processed_data,
                            date_col='Month' if 'Month' in st.session_state.processed_data.columns else None
                        )
                        st.plotly_chart(fig_monthly, use_container_width=True)
                    else:
                        st.warning("âš ï¸ Methode create_monthly_trends nicht gefunden. Bitte App neu starten.")
                except Exception as e:
                    st.warning(f"âš ï¸ Monatliche Trend-Analyse konnte nicht erstellt werden: {e}")
                    st.exception(e)
            
            # Download Option
            csv = df_cleaned.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="ðŸ“¥ Bereinigte Daten als CSV herunterladen",
                data=csv,
                file_name="cleaned_sqp_data.csv",
                mime="text/csv"
            )
            
        except Exception as e:
            st.error(f"âŒ Fehler beim Verarbeiten der Dateien: {str(e)}")
            st.exception(e)

st.divider()

# ============================================================================
# SECTION 2: AI CATEGORIZATION
# ============================================================================
st.header("ðŸ¤– AI-Powered Categorization")

if st.session_state.processed_data is None:
    st.warning("âš ï¸ Bitte lade zuerst Daten hoch.")
else:
    df = st.session_state.processed_data
    
    st.markdown("""
    ### Automatische Kategorisierung
    Die KI kategorisiert automatisch alle Search Queries in logische Produktkategorien.
    """)
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.info(f"ðŸ“Š {len(df)} Search Queries bereit zur Kategorisierung")
    
    with col2:
        # Modell-Auswahl
        model_choice = st.selectbox(
            "AI-Modell",
            ["gpt-4-turbo-preview", "gpt-5.1"],
            index=1 if "gpt-5" in st.session_state.categorizer.model.lower() else 0,
            help="GPT-4 Turbo: Schneller & gÃ¼nstiger. GPT-5.1: PrÃ¤ziser durch Reasoning."
        )
        
        # Reasoning Effort nur fÃ¼r GPT-5.1
        if "gpt-5" in model_choice.lower():
            reasoning_effort = st.selectbox(
                "Reasoning Effort",
                ["low", "medium", "high"],
                index=2,
                help="HÃ¶her = prÃ¤ziser aber langsamer"
            )
        else:
            reasoning_effort = None
        
        batch_size = st.number_input("Batch Size", min_value=20, max_value=200, value=100, step=10, help="GrÃ¶ÃŸere Batches = schneller, aber mehr Tokens")
        parallel_processing = st.checkbox("Parallele Verarbeitung", value=True, help="Mehrere Batches gleichzeitig verarbeiten (schneller)")
        max_workers = st.slider("Max. parallele Requests", min_value=1, max_value=5, value=3, help="Mehr = schneller, aber mehr API-Calls gleichzeitig")
    
    # Debug: Zeige verfÃ¼gbare Spalten
    with st.expander("ðŸ” Debug: VerfÃ¼gbare Spalten"):
        st.write("Spalten im DataFrame:", list(df.columns))
        st.write("Hat 'Search Query':", 'Search Query' in df.columns)
        st.write("Hat 'Search Term':", 'Search Term' in df.columns)
        if 'Search Query' in df.columns:
            st.write("Beispiel Search Query:", df['Search Query'].iloc[0] if len(df) > 0 else "Keine Daten")
    
    if st.button("ðŸš€ Starte AI-Kategorisierung", type="primary"):
        # Aktualisiere Modell falls geÃ¤ndert
        if model_choice != st.session_state.categorizer.model:
            st.session_state.categorizer.model = model_choice
            if reasoning_effort:
                st.session_state.categorizer.reasoning_effort = reasoning_effort
        
        # Finde die richtige Search Query Spalte
        search_col = None
        if 'Search Query' in df.columns:
            search_col = 'Search Query'
            st.info(f"âœ… Verwende Spalte: 'Search Query'")
        elif 'Search Term' in df.columns:
            search_col = 'Search Term'
            st.info(f"âœ… Verwende Spalte: 'Search Term'")
        else:
            st.error("âŒ 'Search Query' oder 'Search Term' Spalte nicht gefunden in den Daten.")
            st.write("VerfÃ¼gbare Spalten:", list(df.columns))
            st.stop()
        
        st.info(f"ðŸ¤– Verwende Modell: {model_choice}" + (f" (Reasoning: {reasoning_effort})" if reasoning_effort else ""))
        
        with st.spinner("ðŸ¤– KI kategorisiert Search Queries... Das kann einen Moment dauern."):
            try:
                search_terms = df[search_col].unique().tolist()
                st.info(f"ðŸ“Š Gefunden: {len(search_terms)} einzigartige Search Queries")
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # PrÃ¼fe ob bereits Kategorien vorhanden sind (Caching)
                existing_categories = {}
                new_search_terms = []
                
                if st.session_state.categories:
                    existing_categories = st.session_state.categories
                    new_search_terms = [term for term in search_terms if term not in existing_categories]
                    cached_count = len(search_terms) - len(new_search_terms)
                    if cached_count > 0:
                        status_text.text(f"ðŸ“¦ {cached_count} bereits kategorisiert, {len(new_search_terms)} neu zu kategorisieren...")
                        progress_bar.progress(cached_count / len(search_terms))
                else:
                    new_search_terms = search_terms
                
                # Kategorisiere nur neue Search Terms
                if new_search_terms:
                    status_text.text(f"ðŸ”„ Kategorisiere {len(new_search_terms)} neue Search Queries...")
                    new_categories = st.session_state.categorizer.categorize_search_terms(
                        new_search_terms,
                        batch_size=batch_size,
                        parallel=parallel_processing,
                        max_workers=max_workers
                    )
                    all_categories = {**existing_categories, **new_categories}
                else:
                    all_categories = existing_categories
                    status_text.text("âœ… Alle Search Queries bereits kategorisiert!")
                
                # Update Progress
                progress_bar.progress(1.0)
                status_text.text("âœ… Kategorisierung abgeschlossen!")
                
                # Debug: Zeige Kategorisierungs-Ergebnisse
                st.write(f"ðŸ“Š Kategorien erhalten: {len(all_categories)}")
                if len(all_categories) > 0:
                    # Zeige erste 5 Kategorien als Beispiel
                    sample_categories = dict(list(all_categories.items())[:5])
                    st.json(sample_categories)
                
                # FÃ¼ge Kategorien zu DataFrame hinzu
                df['Category'] = df[search_col].map(all_categories).fillna('Uncategorized')
                
                # Debug: Zeige wie viele Uncategorized sind
                uncategorized_count = (df['Category'] == 'Uncategorized').sum()
                if uncategorized_count > 0:
                    st.warning(f"âš ï¸ {uncategorized_count} Search Queries wurden als 'Uncategorized' markiert")
                
                st.session_state.categorized_data = df
                st.session_state.categories = all_categories
                
                progress_bar.empty()
                status_text.empty()
                
                st.success(f"âœ… {len(search_terms)} Search Queries kategorisiert!")
                st.rerun()  # Aktualisiere die Seite um Kategorien anzuzeigen
                
            except Exception as e:
                st.error(f"âŒ Fehler bei der Kategorisierung: {str(e)}")
                st.exception(e)
                # Zeige Debug-Informationen
                with st.expander("ðŸ” Debug-Informationen"):
                    debug_info = {
                        "Fehler": str(e),
                        "Modell": st.session_state.categorizer.model if hasattr(st.session_state.categorizer, 'model') else 'N/A',
                        "API Key vorhanden": bool(st.session_state.categorizer.client) if hasattr(st.session_state.categorizer, 'client') else False,
                        "Verwendete Spalte": search_col if 'search_col' in locals() else 'N/A',
                        "Anzahl Search Queries": len(search_terms) if 'search_terms' in locals() else 0,
                        "Anzahl Kategorien erhalten": len(all_categories) if 'all_categories' in locals() else 0
                    }
                    st.json(debug_info)
                    
                    if 'all_categories' in locals() and len(all_categories) > 0:
                        st.write("**Erste 10 Kategorien:**")
                        st.json(dict(list(all_categories.items())[:10]))
    
    # Zeige Kategorien-Ãœbersicht
    if st.session_state.categorized_data is not None:
        st.subheader("ðŸ“Š Kategorien-Ãœbersicht")
        df_cat = st.session_state.categorized_data
        
        if 'Category' in df_cat.columns:
            category_counts = df_cat['Category'].value_counts().reset_index()
            category_counts.columns = ['Category', 'Count']
            category_counts = category_counts.head(20)  # BeschrÃ¤nke auf Top 20 Kategorien
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.dataframe(category_counts, use_container_width=True)
            
            with col2:
                fig = go.Figure(data=[go.Pie(
                    labels=category_counts['Category'],
                    values=category_counts['Count'],
                    hole=0.3
                )])
                fig.update_layout(title="Kategorien-Verteilung", height=400)
                st.plotly_chart(fig, use_container_width=True)
            
            # Download kategorisierte Daten
            csv = df_cat.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="ðŸ“¥ Kategorisierte Daten herunterladen",
                data=csv,
                file_name="categorized_sqp_data.csv",
                mime="text/csv"
            )

st.divider()

# ============================================================================
# SECTION 3: DASHBOARD
# ============================================================================
st.header("ðŸ“Š Visual Intelligence Dashboard")

if st.session_state.categorized_data is None:
    st.warning("âš ï¸ Bitte fÃ¼hre zuerst die AI-Kategorisierung durch.")
else:
    df = st.session_state.categorized_data
    
    # PrÃ¼fe ob Category Spalte existiert
    if 'Category' not in df.columns:
        st.error("âŒ Keine Kategorien gefunden. Bitte fÃ¼hre zuerst die AI-Kategorisierung durch.")
    else:
        # Opportunity Matrix
        st.subheader("ðŸŽ¯ Opportunity Matrix")
        st.markdown("""
        Diese Grafik kombiniert drei Perspektiven auf einen Blick:
        * **X-Achse (Market Volume)** zeigt, wie viele Impressions bzw. Suchanfragen in einer Kategorie anfallen.
        * **Y-Achse (Market Share)** zeigt deinen Anteil an den Sales in dieser Kategorie.
        * **Bubble-GrÃ¶ÃŸe & Farbe** reprÃ¤sentieren deine absoluten Sales.
        
        Interpretation: Kategorien rechts oben sind bereits stark (hohes Volumen, hoher Marktanteil). Spannend fÃ¼r Wachstum sind vor allem Kategorien weit rechts, aber mit niedrigem Market Share â€“ hier existiert viel Nachfrage, aber dein Anteil ist noch gering.
        """)
        
        try:
            # Bereite Daten fÃ¼r Opportunity Matrix vor
            search_col = 'Search Query' if 'Search Query' in df.columns else 'Search Term'
            category_stats = df.groupby('Category').agg({
                'Impressions': 'sum',
                'Sales': 'sum',
                search_col: 'count'
            }).reset_index()
            category_stats.columns = ['Category', 'Total Impressions', 'Total Sales', 'Query Count']
            
            total_sales = category_stats['Total Sales'].sum()
            if total_sales > 0:
                category_stats['Market Share %'] = (
                    category_stats['Total Sales'] / total_sales * 100
                ).round(2)
            else:
                category_stats['Market Share %'] = 0
            
            fig_opportunity = st.session_state.viz_engine.create_opportunity_matrix(
                category_stats,
                category_col='Category',
                volume_col='Total Impressions',
                share_col='Market Share %',
                sales_col='Total Sales'
            )
            st.plotly_chart(fig_opportunity, use_container_width=True)
            
        except Exception as e:
            st.error(f"Fehler beim Erstellen der Opportunity Matrix: {e}")
            st.exception(e)
        
        st.divider()
        
        # Market Trend Analysis
        st.subheader("ðŸ“ˆ Market Trend Analysis")
        st.markdown("""
        Diese Dual-Achsen-Linie zeigt pro Zeitraum oder Kategorie sowohl das **Gesamtvolumen (Impressions)** als auch deine **Sales Velocity**.
        * Die blaue Linie misst, wie sich das Marktvolumen entwickelt â€“ wÃ¤chst die Kategorie oder schrumpft sie?
        * Die orange Linie zeigt, ob deine eigenen Sales Schritt halten, schneller wachsen oder zurÃ¼ckfallen.
        
        Ein Auseinanderlaufen der Linien bedeutet: Das Marktvolumen entwickelt sich anders als dein Sales-Anteil. So erkennst du frÃ¼hzeitig, wo du Marktanteile verlierst oder gewinnst.
        """)
        
        try:
            fig_trend = st.session_state.viz_engine.create_trend_analysis(df)
            st.plotly_chart(fig_trend, use_container_width=True)
        except Exception as e:
            st.error(f"Fehler beim Erstellen der Trend-Analyse: {e}")
        
        st.divider()
        
        # Share of Voice
        st.subheader("ðŸŽ¤ Share of Voice Tracking")
        st.markdown("""
        Der Share-of-Voice-Chart vergleicht pro Kategorie zwei Kennzahlen:
        * **Impression Share %** â€“ wie hÃ¤ufig wirst du angezeigt im Vergleich zum gesamten Markt?
        * **Sales Share %** â€“ wie viel Umsatz generierst du relativ zum Gesamtumsatz der Kategorie?
        
        Wenn der Impression Share deutlich hÃ¶her ist als der Sales Share, gewinnst du zwar Sichtbarkeit, konvertierst aber nicht entsprechend. Umgekehrt deutet ein hÃ¶herer Sales Share darauf hin, dass deine Listings sehr effizient verkaufen. Ideal ist ein ausgewogenes VerhÃ¤ltnis.
        """)
        
        try:
            fig_sov = st.session_state.viz_engine.create_share_of_voice(df)
            st.plotly_chart(fig_sov, use_container_width=True)
        except Exception as e:
            st.error(f"Fehler beim Erstellen der Share of Voice Analyse: {e}")
        
        st.divider()
        
        # Performance Heatmap
        st.subheader("ðŸ”¥ Performance Heatmap")
        st.markdown("""
        Die Heatmap zeigt fÃ¼r jede Kategorie den Durchschnittswert der ausgewÃ¤hlten Metrik (z.â€¯B. CTR, Conversion Rate, Sales). 
        * **Warme Farben** (rot/gelb) stehen fÃ¼r Ã¼berdurchschnittliche Performance.
        * **KÃ¼hle Farben** (grÃ¼n/blau) weisen auf Potenzial oder Problembereiche hin.
        
        So erkennst du sofort, welche Kategorien bei der gewÃ¤hlten Kennzahl herausragen und welche optimiert werden sollten.
        """)
        
        metric_option = st.selectbox(
            "WÃ¤hle Metrik",
            ['CTR', 'Conversion Rate', 'Sales', 'Impressions'],
            key='heatmap_metric'
        )
        
        try:
            fig_heatmap = st.session_state.viz_engine.create_performance_heatmap(
                df,
                category_col='Category',
                metric_col=metric_option
            )
            st.plotly_chart(fig_heatmap, use_container_width=True)
        except Exception as e:
            st.error(f"Fehler beim Erstellen der Heatmap: {e}")

st.divider()

# ============================================================================
# SECTION 4: DEEP DIVE ANALYTICS
# ============================================================================
st.header("ðŸ” Deep Dive Analytics")

if st.session_state.categorized_data is None:
    st.warning("âš ï¸ Bitte fÃ¼hre zuerst die AI-Kategorisierung durch.")
else:
    df = st.session_state.categorized_data
    
    if 'Category' not in df.columns:
        st.error("âŒ Keine Kategorien gefunden. Bitte fÃ¼hre zuerst die AI-Kategorisierung durch.")
    else:
        # Filter-Optionen
        st.subheader("ðŸ”Ž Filter & Suche")
        st.markdown("""
        In der Deep-Dive-Section kannst du jede Kategorie oder einen einzelnen Suchbegriff detailliert analysieren. 
        * Nutze die Filter (Kategorie, Suchtext, minimale Impressions), um den Datensatz einzugrenzen.
        * Sortiere nach der gewÃ¼nschten Kennzahl, um Top-Performer oder AusreiÃŸer zu finden.
        * Die Kennzahlen am Kopf zeigen die aggregierten Werte fÃ¼r deine aktuelle Auswahl.
        
        Ideal fÃ¼r Ad-hoc-Analysen, wenn du konkrete OptimierungsmÃ¶glichkeiten pro Suchbegriff identifizieren willst.
        """)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            categories = ['Alle'] + df['Category'].unique().tolist()
            selected_category = st.selectbox("Kategorie", categories)
        
        with col2:
            search_term_filter = st.text_input("Search Query suchen", "")
        
        with col3:
            min_impressions = st.number_input("Min. Impressions", min_value=0, value=0, step=100)
        
        # Filter anwenden
        df_filtered = df.copy()
        
        if selected_category != 'Alle':
            df_filtered = df_filtered[df_filtered['Category'] == selected_category]
        
        if search_term_filter:
            search_col = 'Search Query' if 'Search Query' in df_filtered.columns else 'Search Term'
            if search_col in df_filtered.columns:
                df_filtered = df_filtered[
                    df_filtered[search_col].str.contains(search_term_filter, case=False, na=False)
                ]
        
        if min_impressions > 0:
            df_filtered = df_filtered[df_filtered['Impressions'] >= min_impressions]
        
        st.info(f"ðŸ“Š {len(df_filtered)} Search Queries gefunden")
        
        # Sortierung
        sort_by = st.selectbox(
            "Sortiere nach",
            ['Impressions', 'Sales', 'Orders', 'CTR', 'Conversion Rate'],
            index=0
        )
        sort_order = st.radio("Reihenfolge", ["Absteigend", "Aufsteigend"], horizontal=True)
        
        ascending = sort_order == "Aufsteigend"
        df_filtered = df_filtered.sort_values(sort_by, ascending=ascending)
        
        # Zeige Ergebnisse
        st.subheader("ðŸ“‹ Search Query Details")
        
        # Wichtige Metriken
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("Total Volume", f"{df_filtered['Impressions'].sum():,.0f}")
        with col2:
            st.metric("Total Sales", f"â‚¬{df_filtered['Sales'].sum():,.2f}")
        with col3:
            st.metric("Total Orders", f"{df_filtered['Orders'].sum():,.0f}")
        with col4:
            st.metric("Avg CTR", f"{df_filtered['CTR'].mean():.2f}%")
        with col5:
            st.metric("Avg CVR", f"{df_filtered['Conversion Rate'].mean():.2f}%")
        
        # Daten-Tabelle
        search_col = 'Search Query' if 'Search Query' in df_filtered.columns else 'Search Term'
        display_cols = [search_col, 'Category', 'Impressions', 'Clicks', 'CTR', 
                       'Orders', 'Sales', 'Conversion Rate']
        available_cols = [col for col in display_cols if col in df_filtered.columns]
        
        st.dataframe(
            df_filtered[available_cols],
            use_container_width=True,
            height=400
        )
        
        # Download
        csv = df_filtered.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ðŸ“¥ Gefilterte Daten herunterladen",
            data=csv,
            file_name="filtered_sqp_data.csv",
            mime="text/csv"
        )

# Footer
st.divider()
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>Amazon Search Query Performance Analyzer | Privacy First | Powered by ChatGPT</p>
</div>
""", unsafe_allow_html=True)
